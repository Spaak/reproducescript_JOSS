
@article{andersenGroupAnalysisFieldTrip2018,
  title = {Group {{Analysis}} in {{FieldTrip}} of {{Time}}-{{Frequency Responses}}: A {{Pipeline}} for {{Reproducibility}} at {{Every Step}} of {{Processing}}, {{Going From Individual Sensor Space Representations}} to an {{Across}}-{{Group Source Space Representation}}},
  shorttitle = {Group {{Analysis}} in {{FieldTrip}} of {{Time}}-{{Frequency Responses}}},
  author = {Andersen, Lau M.},
  year = {2018},
  month = may,
  journal = {Frontiers in Neuroscience},
  volume = {12},
  pages = {261},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00261},
  langid = {english},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/S68SYQGQ/Andersen - 2018 - Group Analysis in FieldTrip of Time-Frequency Resp.pdf}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  langid = {english},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/HTP5D4QF/Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@article{gilmoreProgressOpennessTransparency2017,
  title = {Progress {{Toward Openness}}, {{Transparency}}, and {{Reproducibility}} in {{Cognitive Neuroscience}}},
  author = {Gilmore, Rick O. and Diaz, Michele T. and Wyble, Brad A. and Yarkoni, Tal},
  year = {2017},
  month = may,
  journal = {Annals of the New York Academy of Sciences},
  volume = {1396},
  number = {1},
  pages = {5--18},
  issn = {0077-8923},
  doi = {10.1111/nyas.13325},
  abstract = {Accumulating evidence suggests that many findings in psychological science and cognitive neuroscience may prove difficult to reproduce; statistical power in brain imaging studies is low, and has not improved recently; software errors in common analysis tools are common, and can go undetected for many years; and, a few large scale studies notwithstanding, open sharing of data, code, and materials remains the rare exception. At the same time, there is a renewed focus on reproducibility, transparency, and openness as essential core values in cognitive neuroscience. The emergence and rapid growth of data archives, meta-analytic tools, software pipelines, and research groups devoted to improved methodology reflects this new sensibility. We review evidence that the field has begun to embrace new open research practices, and illustrate how these can begin to address problems of reproducibility, statistical power, and transparency in ways that will ultimately accelerate discovery.},
  pmcid = {PMC5545750},
  pmid = {28464561},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/WN5MRG2R/Gilmore et al. - 2017 - Progress Toward Openness, Transparency, and Reprod.pdf}
}

@article{gleesonCommitmentOpenSource2017,
  title = {A {{Commitment}} to {{Open Source}} in {{Neuroscience}}},
  author = {Gleeson, Padraig and Davison, Andrew P. and Silver, R. Angus and Ascoli, Giorgio A.},
  year = {2017},
  month = dec,
  journal = {Neuron},
  volume = {96},
  number = {5},
  pages = {964--965},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.10.013},
  langid = {english},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/KB4IJRHS/Gleeson et al. - 2017 - A Commitment to Open Source in Neuroscience.pdf}
}

@misc{MATLAB2020,
  title = {{{MATLAB}}},
  year = {2020},
  address = {{Natick, Massachusetts}},
  howpublished = {The Mathworks Inc.}
}

@article{oostenveldFieldTripOpenSource2011,
  title = {{{FieldTrip}}: Open {{Source Software}} for {{Advanced Analysis}} of {{MEG}}, {{EEG}}, and {{Invasive Electrophysiological Data}}},
  shorttitle = {{{FieldTrip}}},
  author = {Oostenveld, Robert and Fries, Pascal and Maris, Eric and Schoffelen, Jan-Mathijs},
  year = {2011},
  journal = {Computational Intelligence and Neuroscience},
  volume = {2011},
  pages = {1--9},
  issn = {1687-5265, 1687-5273},
  doi = {10.1155/2011/156869},
  langid = {english},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/3NTAMM4G/156869 (1).pdf}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  langid = {english},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/5WAAQWWZ/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
  title = {False-{{Positive Psychology}}: Undisclosed {{Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/XKPVG53S/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{szucsEmpiricalAssessmentPublished2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = mar,
  journal = {PLoS Biology},
  volume = {15},
  number = {3},
  issn = {1544-9173},
  doi = {10.1371/journal.pbio.2000797},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64\textendash 1.46) for nominally statistically significant results and D = 0.24 (0.11\textendash 0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience., Biomedical science, psychology, and many other fields may be suffering from a serious replication crisis. In order to gain insight into some factors behind this crisis, we have analyzed statistical information extracted from thousands of cognitive neuroscience and psychology research papers. We established that the statistical power to discover existing relationships has not improved during the past half century. A consequence of low statistical power is that research studies are likely to report many false positive findings. Using our large dataset, we estimated the probability that a statistically significant finding is false (called false report probability). With some reasonable assumptions about how often researchers come up with correct hypotheses, we conclude that more than 50\% of published findings deemed to be statistically significant are likely to be false. We also observed that cognitive neuroscience studies had higher false report probability than psychology studies, due to smaller sample sizes in cognitive neuroscience. In addition, the higher the impact factors of the journals in which the studies were published, the lower was the statistical power. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  pmcid = {PMC5333800},
  pmid = {28253258},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/JCYPKSY2/Szucs and Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf}
}

@article{zwaanMakingReplicationMainstream2017,
  title = {Making {{Replication Mainstream}}},
  author = {Zwaan, Rolf A. and Etz, Alexander and Lucas, Richard E. and Donnellan, M. Brent},
  year = {2017},
  month = oct,
  journal = {Behavioral and Brain Sciences},
  pages = {1--50},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X17001972},
  langid = {english},
  file = {/Volumes/T5_OHBA/Literature/Zotero/storage/ZGV7P6F3/Zwaan e.a. - 2017 - MAKING REPLICATION MAINSTREAM.pdf}
}


